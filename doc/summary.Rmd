---
title: "Processing Nodes 4 VAIA"
author: "Francesco Pirotti"
date: "11/7/2020"
bibliography: bib.bibtex
output:
  html_document: 
    toc: true 
    toc_float: true
---
 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


This effort is also part of the [VAIA FRONT project - FRom lessong learned to future Options ](https://www.tesaf.unipd.it/ricerca/progetti-dip-tesaf) . 

<img src="img/vaia_front.svg" />
 
## Summary
We have to assign land-cover attributes to nodes over a large area for simulating the impact of strong winds over forest areas.

Several layers are processed to aggregate information over nodes of a regular grid.

Data were processed with Google Earth Engine (GEE) and R.

### Code and outputs

Code is reported below (snippets) and available in [Github](https://github.com/fpirotti/WRF_UConn) . 

Outputs are available at  [Github   page in the folder "out"](https://github.com/fpirotti/WRF_UConn/tree/master/out).


### Forest DAMAGES layer  

Training and validation was done over areas that were defined over areas that were damaged from the VAIA storm event.  

The data is extracted from the windthrow database that was created for the paper by  [[@Forzieri2020]](https://essd.copernicus.org/articles/12/257/2020/) linking to an [open database](https://figshare.com/articles/A_spatially-explicit_database_of_wind_disturbances_in_European_forests_over_the_period_2000-2018/9555008) containing polygons of damaged areas. It must be noted that some damaged areas were left out because only slightly damaged, or with small areas or outright missed during the visual interpretation process. Therefore we should be careful to conclude that all areas not covered by the polygons are un-damaged. Nevertheless these data provide us with samples of damaged areas. 

There are 7416 polygons in the dataset: 5235 have information on the degree of damage, expressed between 0 and 1, 2181 have NULL value (expressed as -9999).



## Aggregating data at nodes
A regular grid with 1 km spacing was used over the study area, for a total of 97524 nodes.     

**N.B.** A nice alternative  could be to have an hexagonal grid, which has several geometric advantages.   
  Cannot convert our regular grid to hexagonal grid because  hexagonal requires offset at alternate rows.   

Grid is obviously not aligned to a regular latitude longitude grid as distance between nodes would decrease moving away from the equator.  

A custom projection Lambert Conformal Conical projection was designed (Marika Koukoula) with the following PROJ 
 

```{r message=F }
myproj<-"+proj=lcc +lat_1=45.827  +lat_2=45.827  +lat_0=45.827 +lon_0=11.625 +x_0=4000000 +y_0=2800000 +ellps=GRS80"

```

Each node was expanded to a square

```{r eval=FALSE} 
  
  st_bbox_by_feature = function(x) {
    x = st_geometry(x)
    f <- function(y) st_as_sfc(st_bbox(y,), crs=myproj)
    do.call("c", lapply(x, f))
  }
  
    
  nodes.myproj.buffered<-st_buffer(nodes.myproj, 500, nQuadSegs = 1)
  squares<-st_bbox_by_feature(nodes.myproj2)
  squares <- squares  %>% st_set_crs( myproj)
  
```

The square lattice was used for further processing with Google Earth Engine and with R.

### Google Earth Engine processing

The 97524 square polygons were uploaded to Google Earth Engine for processing raster products derived from satellite data.   

The grid was used to extract  of the following two files:  

+  **copernicus.csv**    
Dictionary with histogram of class frequency of Copernicus classes inside the 1 km x 1 km area square around the nodes of the grid (domain?). Extracted using Google Earth Engine map/reduce over [2018 COPERNICUS land cover](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_CORINE_V20_100m)   

+  **nodesWithGEEvars_crsXXXX**    
  XXXX = 4326 (latitude longitude in WGS88) or
  XXXX=LCCcustom (custom Lambert Conformal Conical <a href="#Aggregating_data_at_nodes" >see here</a> ) ) 
  with percentiles [10,25,50,75,90] of the following as attributes. 
  A shapefile with nodes and the five percentiles for each of the following attributes.
  
  
      + Tree canopy cover for year 2000, defined as canopy closure for all vegetation taller than 5m in height. (https://developers.google.com/earth-engine/datasets/catalog/UMD_hansen_global_forest_change_2018_v1_6) Hansen, Potapov, Moore, Hancher et al. “High-resolution global maps of 21st-century forest cover change.” Science 342.6160 (2013): 850-853.

    + Tree canopy height for year 2005, defined as canopy closure for all vegetation taller than 5m in height. (https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2011JG001708) 
 
    + Aspect

    + Slope 

    + DEM (Height) 
  
    The last three from the SRTM mission. Slope and aspect in degrees. Height is meters A.S.L. (https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2011JG001708) 

The following code was used in GEE.

```{python eval=F} 

var getCentroid = function(feature){
  return feature.centroid();
};  
 
var slp = ee.Terrain.slope(srtm).clip(domain_buffered.geometry().bounds()) ;
var asp = ee.Terrain.aspect(srtm).clip(domain_buffered.geometry().bounds()) ;
var dem =  srtm.clip(domain_buffered.geometry().bounds()) ;
 
var image = hansen.select(['treecover2000']).rename(['cCov'])
            .addBands(canopyHeight.rename(['cHgt']))
            .addBands(slp.rename(['slp']))
            .addBands(asp.rename(['asp']))
            .addBands(dem.rename(['dem'])) ;


var  treeCover =  image.reduceRegions({
  reducer: ee.Reducer.percentile([10,25,50,75,90]),  
  collection:domain_buffered 
}); 
 

var  unBuffer = ee.FeatureCollection(treeCover.map(getCentroid));
 
 Export.table.toDrive({
    collection: unBuffer,
    description:'mapRedVars',
    fileFormat: 'SHP'
  });

 
```



### R-CRAN processing

Processing 97524 squares intersecting them with 121924 polygons of forest category and 7416 polygons of damage areas too 120 seconds with 14 CPU threads parallel tasking (not sure if this is the correct terminology - not a parallel guru).

#### Variables 

Table with variables: 
**asp**=aspect, **slp**=slope, **dem**=DEM, **cHgt**=canopy height, **cCv**=canopy cover, **mainCat**=main category, **mnCt_Cv**=main category cover, **dmg_Cov**=damage cover, **dmg_wCov**=damage weighted cover

All cover values are in m^2 and can be converted to relative cover dividing by the square area (1 km^2).


```{r message=F, comment=F, echo=F}
 
library(sf)  
library(knitr)
library(mapview)
library(kableExtra)


#study.area<-st_read("/archivio/shared/geodati/vettoriali/WRF_UConn/area.shp")
final.file<-"../out/fromGEEreducedVars/nodesWithGEEvars_andRvar_crsLCCcustom.shp"

if(file.exists(final.file) && 
   (!file.exists("Units.rds") || 
    !file.exists("final.nodes.binde.rds") || 
    !file.exists("damages.clipped.rds") ) ){

   
  saveRDS(st_read("/archivio/shared/geodati/vettoriali/WRF_UConn/area.shp"), "../study.area.rds")
 
  saveRDS( st_read(final.file), "final.nodes.binde.rds")  
  
  ss<- st_intersects(final.nodes.binded, study.area )
  
  geomes <-     sf::st_make_valid(damages.for.myproj) 
  
  saveRDS( st_intersection(geomes, study.area ), "damages.clipped.rds")  
  
  nodes.intersecting.polys.ids <-which (lengths(ss) > 0 )
  Units<-final.nodes.binded[nodes.intersecting.polys.ids,]
  Units.sq<-  cbind( squares[nodes.intersecting.polys.ids,], Units)
  saveRDS(Units,"Units.rds")
  saveRDS(Units.sq,"Units.sq.rds")
  
} 

  squares<-readRDS( "../squares.RDS")
  Units.sq<-readRDS( "Units.sq.rds")
  Damage <- readRDS("damages.clipped.rds")
  final.nodes.binded<-readRDS("final.nodes.binde.rds")
  Units<-readRDS("Units.rds")
  
tt<-lapply( names(final.nodes.binded), function(x){
  if( !(x %in% c("FID", "mainCat", "rown", "geometry")))
    list(`Column Name`=x, Type=class(final.nodes.binded[[x]]), 
         Min=round( min(final.nodes.binded[[x]], na.rm =T ), 2), 
         Mean=round( mean(final.nodes.binded[[x]], na.rm =T ), 2), 
         Max=round( max(final.nodes.binded[[x]], na.rm =T ), 2) )
  else 
    NULL
})



dtt<-data.table::rbindlist(tt)

dtt<-dtt[order(dtt$`Column Name`),]

dttp<-dtt %>%
  kable(format.args = list(big.mark = " ")) %>%
  kable_styling()

dttp



 
# + mapview(st_centroid(squares[squares.intersecting.polys.ids,] ), label = NULL)
# st_centroid(ss) %>% mm
```

### Map of subset of study area 

```{r   message=F,   echo=F}

Damage$Damage_deg [ Damage$Damage_deg<0 ] <- 1
Units.sq$dmg_wCv <- round(Units.sq$dmg_wCv/1000000*100, 1)
Units.sq$dmg_Cov <- round(Units.sq$dmg_Cov/1000000*100, 1)

label <- sprintf("Damage: Cover (%%): %s |
Weighted Cover (%%): %s<br>",
                 Units.sq$dmg_Cov,
                 Units.sq$dmg_wCv)
#mapview(Units, alpha = 0, na.color=NULL, zcol = "dmg_wCv" ) +

mapview(Damage, alpha = 0, na.color=NA, label=NULL, at = seq(0, 1, 0.2),  zcol = "Damage_deg" )  +
mapview(Units.sq, alpha = 0, na.color="#FFFFFFFF", label= label, lwd=0, zcol ="dmg_wCv" ,
        at = seq(0, 100, 10), legend = TRUE) 
 
 



```
 


### Forest area and forest categories   
Italy is divided into regions and each region into provinces. We have a full cover of forest categories of two regions, Veneto and Friuli Venezia Giulia, and the province of Trento.   Categories slightly differ in classification, but discrepancies were merged into a single catogory.    
Classification can provide us with a binary result: positives/negatives = damaged/undamaged forest areas - with the following alternative outputs per each unit:

+ **true  positives** = classified as damaged  correctly
+ **false positives** = classified as damaged  but not true
+ **true negatives** =  classified as undamaged  correctly
+ **false negatives** = classified as undamaged  but are actually damaged



